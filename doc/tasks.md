## Task 1 - Looking at the LLVM IR
Inspect the LLVM intermediate representation (IR) generated from the examples in the [llvm-pass-skeleton/samples](https://github.com/rovka/llvm-pass-skeleton/tree/master/samples) directory ([hello.c](https://github.com/rovka/llvm-pass-skeleton/blob/master/samples/hello.c), [sum.c](https://github.com/rovka/llvm-pass-skeleton/blob/master/samples/sum.c), [bpk.c](https://github.com/rovka/llvm-pass-skeleton/blob/master/samples/bpk.c), etc.):

    clang -Xclang -disable-O0-optnone -S -emit-llvm in.c -o out.ll

Try to understand how each statement in the input C file is mapped into LLVM IR.

**TODO 1**: You can open the output file in a text editor, or you can look at the control-flow graph (CFG) generated by `opt` (as you can imagine, the latter is particularly useful for visualising the high level structure of the IR - basic blocks and the relationships between them):

    opt -dot-cfg in.ll -o /dev/null ; produces one dot file per function
    xdot cfg.f.dot

or:

    opt -view-cfg in.ll -o /dev/null

**TODO 2**: The IR that you are looking at after running these commands is the most naive IR that can be generated. Most optimizations don’t work with this, but instead expect to see IR in SSA form. You can obtain this by invoking `opt` with the `mem2reg` pass.

    opt -mem2reg in.ll -S -o out.ll

Feel free to look at the CFGs before and after if you prefer:

    opt -view-cfg -mem2reg -view-cfg in.ll -o /dev/null

Make sure you have passed `-Xclang -disable-O0-optnone` to `clang`, otherwise you probably won’t be seeing any difference.

## Task 2 - Automatic Vectorization
For this exercise, we’re going to work with the bpk.c sample.

**TODO 1**: Get the LLVM IR corresponding to [bpk.c](https://github.com/rovka/llvm-pass-skeleton/blob/master/samples/bpk.c) and run the loop vectorization pass on it (don’t forget to pass `-Xclang -disable-O0-optnone`):

    opt -mem2reg -loop-vectorize -view-cfg bpk.ll -o /dev/null

How was the IR transformed?

**TODO 2**: We can find out more info about what the pass is doing by using the `-pass-remarks-analysis` flag:

    opt -mem2reg -loop-vectorize -view-cfg -pass-remarks-analysis=loop-vectorize bpk.ll -o /dev/null

You can also obtain extra info with `-debug-only=loop-vectorize`.

**TODO 3**: Running with `-pass-remarks-analysis` should tell us that it doesn’t understand the loop control flow. This is a bit difficult to guess without more intimate knowledge on how the loop vectorizer works, but the problem is that the loop is not in a canonical form that the vectorizer can understand. Canonicalization happens often inside a compiler, in order to keep the complexity of the passes low. In this case, we canonicalize with the loop rotate pass.

    opt -mem2reg -loop-rotate -loop-vectorize -view-cfg bpk.ll -o /dev/null

How was the IR transformed this time? You can also look at the IR right after loop rotate to see what form the loops need to be in so the loop vectorizer can work with them.

## Task 3 - Analysing the LLVM IR
With this exercise we’re going to start diving into the LLVM APIs. Use the code in [llvm-pass-skeleton/skeleton/Skeleton.cpp](https://github.com/rovka/llvm-pass-skeleton/blob/master/skeleton/Skeleton.cpp). This currently contains a skeleton for a pass. It will be compiled into a shared library that can be loaded into `clang` or `opt`.

Building the shared library:

    cd llvm-pass-skeleton/build
    ninja SkeletonPass

Running the skeleton pass:

    clang -Xclang -load \
    -Xclang ~/llvm-pass-skeleton/build/skeleton/libSkeletonPass.so \ ~/llvm-pass-skeleton/samples/fib.c -o /dev/null \
    -mllvm -debug-only=skeleton

or (after you obtain `fib.ll`)

    opt -load ~/llvm-pass-skeleton/build/skeleton/libSkeletonPass.so \
    -skeleton -debug-only=skeleton path/to/fib.ll -o /dev/null

You can also play with any of the other samples. When running the pass, you should see one message for every function in the input module.

**TODO 1**: Update the code in [Skeleton.cpp](https://github.com/rovka/llvm-pass-skeleton/blob/master/skeleton/Skeleton.cpp) to go through all the basic blocks in the function and dump all the instructions. Use the LLVM API cheatsheet for help.

For convenience, you can rebuild your pass and run it on the [fib.c](https://github.com/rovka/llvm-pass-skeleton/blob/master/samples/fib.c) sample by using the fib target:

    cd llvm-pass-skeleton/build
    ninja fib
    vim samples/fib-opt.ll (or opt -view-cfg samples/fib-opt.ll)

Note that this target doesn’t pass the `-debug-only` flag, so don’t use the `DEBUG` macro when dumping your instructions.

**TODO 2**: Instead of printing all the instructions, print only the calls to functions.

**TODO 3**: Narrow the analysis even more by printing only calls to the `fib` function with constant arguments. You can use the `isFibFunction` helper to check if a function is the `fib` function with the signature that we expect.

## Task 4 - Transforming the IR

After identifying all the calls to `fib` with constant arguments in the previous task, we can now do something about them - since the argument is constant, we can know the result at compile time, and replace the function call with this result.

**TODO 1**: Dump the value of the constant argument.

**TODO 2**: Obtain and dump the Fibonacci number corresponding to that argument - you can use the `fib` helper from the code skeleton.

**TODO 3**: Replace all the uses of the call instruction with a `ConstantInt` representing the Fibonacci number that you just computed.

**Bonus TODO**:
Notice that although we have replaced all the uses of the call instruction, the call remains in the IR, which means the function will still be called even though its result is no longer used. You should remove these calls.

*Option 1*:
Remove the calls yourself in the same pass, after replacing their uses. The caveat here is that removing an instruction will invalidate the iterators, so you shouldn’t remove in the middle of a loop over the instructions in the basic block. Instead, you should save the instructions that you want to remove in some data structure (a `std::vector` is fine, you can also read about other data structures in the [LLVM Programmer’s Manual](https://releases.llvm.org/5.0.0/docs/ProgrammersManual.html)), and then loop over that data structure and remove the instructions.

*Option 2*:
Try to use the Aggressive Dead Code Elimination pass (`-adce`) to remove the calls. Why doesn’t it work out of the box? Try to follow the code in [lib/Transforms/Scalar/ADCE.cpp](https://github.com/llvm-mirror/llvm/blob/release_50/lib/Transforms/Scalar/ADCE.cpp).

## Task 5 - Static Call Graph
For this task we will use the code skeleton in [llvm-pass-skeleton/static-callgraph/StaticCallGraph.cpp](https://github.com/rovka/llvm-pass-skeleton/blob/master/static-callgraph/StaticCallGraph.cpp). This contains a pass skeleton which should dump a dot file representing the call graph of the module, which is a graph of the functions in the module with edges from each function to the functions that it may call.

For this we use a library consisting of 3 functions:

    void print_prologue();

    void print_epilogue();

    void print_edge(const char * src, const char * dst);

The usage of the library is simple - one must call `print_prologue` to open the output file and dump a prologue, followed by a number of `print_edge` calls to represent the edges from the caller to the callee, followed by one final `print_epilogue` to dump the epilogue and close the output file. The output file is hardcoded to `callgraph.dot`. The code skeleton currently calls `print_prologue` and `print_epilogue` for you.

Building the shared library:

    cd llvm-pass-skeleton/build
    ninja StaticCallGraphPass

Running the static callgraph pass:

    clang -Xclang -load -Xclang \ ~/llvm-pass-skeleton/build/static-callgraph/libStaticCallGraphPass.so \
    ~/llvm-pass-skeleton/samples/d.c -o /dev/null

or (after you obtain `d.ll`):

    opt -load \
    ~/llvm-pass-skeleton/build/static-callgraph/libStaticCallGraphPass.so \
    -static-call-graph path/to/d.ll -o /dev/null

For convenience, you can use the shortcut target:

    ninja static-cg
    xdot samples/static-callgraph.dot

**TODO**: Find all the call instructions and call `print_edge` with the name of the caller function and the name of the callee function.

Feel free to compare with

    opt -view-callgraph samples/d.ll

## Task 6 - Dynamic Call Graph
For this task we will use the code skeleton in [llvm-pass-skeleton/dynamic-callgraph/DynamicCallGraph.cpp](https://github.com/rovka/llvm-pass-skeleton/blob/master/dynamic-callgraph/DynamicCallGraph.cpp). We will try to obtain the dynamic call graph instead of the static one - this is a subset of the static call graph that we obtained in the previous task, but containing only the calls that were actually performed when running the executable.

For this, we will instrument the IR so that the call graph library is used at runtime by the program that we are compiling. Before each call instruction, we will insert code that calls the `print_edge` function. We must also make sure that `print_prologue` is called right after the program begins execution, and `print_epilogue` is called right before the program exits.

Building the shared library:

    cd llvm-pass-skeleton/build
    ninja DynamicCallGraphPass

Running the dynamic call graph pass:

    clang -Xclang -load -Xclang \ ~/llvm-pass-skeleton/build/dynamic-callgraph/libDynamicCallGraphPass.so \
    ~/llvm-pass-skeleton/build/utils/callgraph/libcallgraph.so \
    -Wl,-rpath,~/llvm-pass-skeleton/build/utils/callgraph \
    ~/llvm-pass-skeleton/samples/d.c -o d

or (after you obtain `d.ll`):

    opt -load \
    ~/llvm-pass-skeleton/build/dynamic-callgraph/libDynamicCallGraphPass.so \
    -dynamic-call-graph path/to/d.ll -S -o d-instr.ll

    clang d-instr.ll -o d \
    ~/llvm-pass-skeleton/build/utils/callgraph/libcallgraph.so \
    -Wl,-rpath,~/llvm-pass-skeleton/build/utils/callgraph

Notice that we now have to obtain an executable, and we also have to link the `callgraph` library into it (if you install it on your system, it's enough to use `-lcallgraph`, there's no need for the `rpath` magic). We can obtain different dynamic call graphs from different invocations of the executable, for instance:

    ./d
    xdot callgraph.dot ; Call graph corresponding to the error path
    ./d ~/llvm-pass-skeleton
    xdot callgraph.dot ; Call graph corresponding to a normal execution path

For convenience, we have two shortcut targets:

    ninja dynamic-cg-normal

    ninja dynamic-cg-error

**TODO 1**: At the moment, the code skeleton does two important things: first, it adds the declarations for `print_prologue` and `print_epilogue` to the module. Without these declarations we cannot insert calls to the functions. Second, it introduces a call to `print_prologue` at the beginning of the main function. This will create a malformed dot file - to fix this, you will have to introduce calls to `print_epilogue` before every return from main (look for `ReturnInst` and for calls to `_exit`). You can reuse the same `IRBuilder` or create a new one, it’s all the same for the purposes of the workshop.

**TODO 2**: We should now have a properly formed, albeit empty, dot file. Time to populate it. The first thing that you need to do is add the declaration for `print_edge` to the module. Follow the example for `print_prologue`/`print_epilogue` but bear in mind that `print_edge` has a different signature - it takes two `char*` parameters.

**TODO 3**: Identify all call instructions and introduce calls to `print_edge` before them. Note that you will have to create the strings that you need to pass to `print_edge`, representing the names of the functions. These should be some global variables storing the names, and you should pass pointers to them to `print_edge`. Make sure you don’t add calls to `print_edge` for any of the calls that we have added as part of the instrumentation (`print_prologue`, `print_epilogue`).

## Bonus Task - `snprintf` Optimization

The `snprintf` function can be used to print formatted strings into buffers:

    int snprintf(char *s, size_t n, const char *format, ...);

The format is the same as the one used for `printf` and may contain format specifiers, which tell the function how to format the additional arguments into the buffer. Parsing and processing the format is done at runtime, but most of the time the format used is a constant string. Therefore, some of that runtime computation could be performed by the compiler, and the call to `snprintf` could be replaced with a series of simpler calls such as `strncpy`, `strncat`, etc.

Write a pass that performs this optimization for format strings that contain only the `%s` specifier. Use [samples/snprintf.c](https://github.com/rovka/llvm-pass-skeleton/blob/master/samples/snprintf.c) as input.